# ARM(Abstract Reasoning Model) 아키텍처 설정
# 전두엽, 시상, 추론 모듈이 명확히 분리되고 안정적인 성장 메커니즘이 추가된 버전
name: arm.arm_v1@ARM # 모델 클래스 경로는 유지 (향후 arm_v1.py 파일이 수정될 것임)

loss:
  name: losses@ARMLossHead
  loss_type: stablemax_cross_entropy

# ACT (Adaptive Computation Time) 관련 설정
halt_max_steps: 16
halt_exploration_prob: 0.1

# --- 모듈별 아키텍처 하이퍼파라미터 ---
frontal_module:
  num_layers: 2
  hidden_size: 256
  num_heads: 4
  expansion: 4

thalamus_module:
  hidden_size: 512
  num_layers: 2

reasoning_module:
  num_layers: 1      # 각 추론 모듈 내부의 트랜스포머 레이어 수
  hidden_size: 256
  num_heads: 4
  expansion: 4
  inner_loops: 5     # 활성 추론 모듈의 내부 재귀 연산 횟수

# --- 공통 Transformer 파라미터 ---
pos_encodings: rope

# --- 동적 성장 메커니즘 하이퍼파라미터 ---
initial_modules: 1                 # 시작 시 추론 모듈의 개수
max_modules: 8                     # 최대 추론 모듈의 개수
convergence_check_interval: 500   # 시스템 수렴 상태를 확인할 주기 (훈련 스텝)
stable_threshold: 2e-8           # 수렴으로 판단할 그래디언트 분산의 임계값
rate_hardprob: 0.1 #0.2               # 고난도 문제로 정의할 상위 n% (0.15 = 15%)
stabilization_duration: 7000       # 새 모듈 추가 후 안정화 학습을 진행할 기간 (훈련 스텝)

# --- 적응 학습 (Adaptive Learning) 메커니즘 하이퍼파라미터 ---
adaptive_learning:
  duration: 4000 #3000                  # 안정화 학습 후 적응 학습을 진행할 기간 (훈련 스텝)
  exploration_prob: 0.1            # 탐험을 위해 무작위 모듈을 선택할 확률 (10%)
  new_module_bonus: -0.3 #-0.3           # 가장 최근 모듈의 예측 오류에 더해줄 보너스 점수 (음수)

# --- Gating을 위한 강화학습(REINFORCE) 하이퍼파라미터 ---
gating_rl:
  reward_baseline_decay: 0.99      # 보상의 기준선(baseline)을 계산할 때 사용하는 지수 이동 평균 감쇠율
  reward_scaling: 1.5             # 최종 보상에 곱해지는 스케일링 팩터